{"cells":[{"metadata":{"trusted":true,"_uuid":"29b54fbf6ce54459d1d5f8b03b3dde9ed2b9c6a8"},"cell_type":"markdown","source":"2018-10-26 08:16 시작  \nGPU on"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport string\nimport re\n\nimport gensim\nfrom collections import Counter\nimport pickle\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import CuDNNLSTM, CuDNNGRU\nfrom keras.preprocessing import text, sequence\n\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras.layers import Lambda\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom nltk.corpus import stopwords\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\nimport gc\nfrom keras import backend as K\nfrom sklearn.model_selection import KFold\n\nfrom unidecode import unidecode\n\nimport time\n\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5759733d62cc0659934afad9e5c56a23daadeae9"},"cell_type":"code","source":"print(\"test\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 1. preprocessing\ntrain = pd.read_csv(\"../input/processing-helps-boosting-about-0-0005-on-lb/train_processed.csv\")\ntest = pd.read_csv(\"../input/processing-helps-boosting-about-0-0005-on-lb/test_processed.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b04e352-b723-4287-8b17-d3f9be960f5c","_uuid":"b4475b7eaffa730fc571a442caeeb565245e210d","trusted":true},"cell_type":"code","source":"#2.  remove non-ascii\n\nspecial_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean\n\ntrain['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\ntest['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))\n\nX_train = train['clean_text'].fillna(\"something\").values\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_test = test['clean_text'].fillna(\"something\").values\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4afa12c-f7cc-4c71-877b-ed7957878cdb","_uuid":"dd51878df2b540f4ca21269d5df60332c56e2c27","trusted":true},"cell_type":"code","source":"def add_features(df):\n    \n    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n    df['total_length'] = df['comment_text'].apply(len)\n    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                axis=1)\n    df['num_words'] = df.comment_text.str.count('\\S+')\n    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nfeatures = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\ntest_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n\nss = StandardScaler()\nss.fit(np.vstack((features, test_features)))\nfeatures = ss.transform(features)\ntest_features = ss.transform(test_features)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"602faa61-83bd-463f-a221-692b760809c7","_uuid":"9bb33ed2eaf58bdb48ad1e85d5467c9bd8887bcb","trusted":true},"cell_type":"code","source":"# For best score (Public: 9869, Private: 9865), change to max_features = 283759, maxlen = 900\nmax_features = 10000\nmaxlen = 50\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d3b0728-bc6c-46e9-96ea-bf216d4c1891","_uuid":"8fe8a034e5f0da11c1f3af5afe8b080e624e9c95","trusted":true},"cell_type":"code","source":"# Load the FastText Web Crawl vectors\nEMBEDDING_FILE_FASTTEXT=\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nEMBEDDING_FILE_TWITTER=\"../input/glove-twitter-27b-200d-txt/glove.twitter.27B.200d.txt\"\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\nembeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec916961-60af-4e30-b3a8-5f3990b76681","_uuid":"73db6660e9f3380335b63d8432a8574e947a5b4f","trusted":true},"cell_type":"code","source":"spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a14b1e3-eff5-4079-b5cc-d485a9fe0f24","_uuid":"f27accdd49c359f12521bbac2c6e5500f371d3f0","trusted":true},"cell_type":"code","source":"# This code is  based on: Spellchecker using Word2vec by CPMP\n# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n\nwords = spell_model.index2word\n\nw_rank = {}\nfor i,word in enumerate(words):\n    w_rank[word] = i\n\nWORDS = w_rank\n\n# Use fast text as vocabulary\ndef words(text): return re.findall(r'\\w+', text.lower())\n\ndef P(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - WORDS.get(word, 0)\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef singlify(word):\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5cb1175-0d64-40aa-ba8e-844478cc0f54","_uuid":"38a44f6c963283dac6d1cefc46d186b4c301b568","trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words,501))\n\nsomething_tw = embeddings_index_tw.get(\"something\")\nsomething_ft = embeddings_index_ft.get(\"something\")\n\nsomething = np.zeros((501,))\nsomething[:300,] = something_ft\nsomething[300:500,] = something_tw\nsomething[500,] = 0\n\ndef all_caps(word):\n    return len(word) > 1 and word.isupper()\n\ndef embed_word(embedding_matrix,i,word):\n    embedding_vector_ft = embeddings_index_ft.get(word)\n    if embedding_vector_ft is not None: \n        if all_caps(word):\n            last_value = np.array([1])\n        else:\n            last_value = np.array([0])\n        embedding_matrix[i,:300] = embedding_vector_ft\n        embedding_matrix[i,500] = last_value\n        embedding_vector_tw = embeddings_index_tw.get(word)\n        if embedding_vector_tw is not None:\n            embedding_matrix[i,300:500] = embedding_vector_tw\n\n            \n# Fasttext vector is used by itself if there is no glove vector but not the other way around.\nfor word, i in word_index.items():\n    \n    if i >= max_features: continue\n        \n    if embeddings_index_ft.get(word) is not None:\n        embed_word(embedding_matrix,i,word)\n    else:\n        # change to > 20 for better score.\n        if len(word) > 0:\n            embedding_matrix[i] = something\n        else:\n            word2 = correction(word)\n            if embeddings_index_ft.get(word2) is not None:\n                embed_word(embedding_matrix,i,word2)\n            else:\n                word2 = correction(singlify(word))\n                if embeddings_index_ft.get(word2) is not None:\n                    embed_word(embedding_matrix,i,word2)\n                else:\n                    embedding_matrix[i] = something     ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77676ed2-6a21-4c89-8a6c-90c502079f2d","_uuid":"30cf8af108c8e43539847cddcd728ab7a8214f62","trusted":true},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        self.max_score = 0\n        self.not_better_count = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            if (score > self.max_score):\n                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n                model.save_weights(\"best_weights.h5\")\n                self.max_score=score\n                self.not_better_count = 0\n            else:\n                self.not_better_count += 1\n                if self.not_better_count > 3:\n                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n                    self.model.stop_training = True","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cbc38b37-1c92-4e69-b444-4e1c6ed90a09","_uuid":"54e58c905ec4916f7f9a492d823f06f4d4b76229","trusted":true},"cell_type":"code","source":"def get_model(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=501):\n    features_input = Input(shape=(features.shape[1],))\n    inp = Input(shape=(maxlen, ))\n    \n    # Layer 1: concatenated fasttext and glove twitter embeddings.\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    # Uncomment for best result\n    #Layer 2: SpatialDropout1D(0.5) #yk\n    x = SpatialDropout1D(dropout)(x) #yk\n    \n    # Uncomment for best result\n    #Layer 3: Bidirectional CuDNNLSTM #yk\n    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x) #yk\n\n\n    # Layer 4: Bidirectional CuDNNGRU\n    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n    \n    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    x = concatenate([avg_pool, x_h, max_pool,features_input])\n    \n    # Layer 6: output dense layer.\n    outp = Dense(6, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=[inp,features_input], outputs=outp)\n    adam = optimizers.adam(clipvalue=clipvalue)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"663b6d15-7071-483c-a58c-d13c1bfe8fb8","_uuid":"518f6fd237f88d8c15368f2b70f17e1cd2f87445","trusted":true,"scrolled":true},"cell_type":"code","source":"\nmodel = get_model(features)\n\nbatch_size = 32\n\n# Used epochs=100 with early exiting for best score.\nepochs = 10 # yk\ngc.collect()\nK.clear_session()\n\n# Change to 10 from 2\nnum_folds = 10 #number of folds, yk\n\npredict = np.zeros((test.shape[0],6))\n\n# Uncomment for out-of-fold predictions\nscores = []\noof_predict = np.zeros((train.shape[0],6))\n\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n\nfor train_index, test_index in kf.split(x_train):\n    \n    kfold_y_train,kfold_y_test = y_train[train_index], y_train[test_index]\n    kfold_X_train = x_train[train_index]\n    kfold_X_features = features[train_index]\n    kfold_X_valid = x_train[test_index]\n    kfold_X_valid_features = features[test_index] \n    \n    gc.collect()\n    K.clear_session()\n    \n    model = get_model(features)\n    \n    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n    \n    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n             callbacks = [ra_val])\n    gc.collect()\n    \n    #model.load_weights(bst_model_path)\n    model.load_weights(\"best_weights_yk_181025.h5\")\n    \n    predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) / num_folds\n    \n    gc.collect()\n    #uncomment for out of fold predictions\n    oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n    cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n    \n    scores.append(cv_score)\n    print('score: ',cv_score)\n\nprint(\"Done\")\nprint('Total CV score is {}'.format(np.mean(scores)))    \n\n\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsample_submission[class_names] = predict\nsample_submission.to_csv('yk_model_9872_baseline_submission.csv',index=False)\n\n# uncomment for out of fold predictions\noof = pd.DataFrame.from_dict({'id': train['id']})\nfor c in class_names:\n   oof[c] = np.zeros(len(train))\n#    \noof[class_names] = oof_predict\nfor c in class_names:\n   oof['prediction_' +c] = oof[c]\noof.to_csv('yk_oof-model_9872_baseline_submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52b9b444-4d2a-4507-9965-224ae88d52fb","_uuid":"f269d6206590f6f40b6a6524d411b5b579546f97","trusted":true},"cell_type":"code","source":"!ls \n","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}